# elderly_biometrics_app.py

import streamlit as st
import os
import cv2
import numpy as np
import librosa
import soundfile as sf
import sounddevice as sd
from keras.models import Sequential, load_model
from keras.layers import LSTM, Dense
from keras_vggface.vggface import VGGFace
from keras_vggface.utils import preprocess_input
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder

# ------------------------- Paths -------------------------
DATA_FACE = 'data/face'
DATA_VOICE = 'data/voice'
MODEL_PATH = 'models/voice_model.h5'

os.makedirs(DATA_FACE, exist_ok=True)
os.makedirs(DATA_VOICE, exist_ok=True)
os.makedirs('models', exist_ok=True)

# ------------------- Face Utilities ----------------------
face_model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')

def extract_face_embedding(img_path):
    img = cv2.imread(img_path)
    img = cv2.resize(img, (224, 224))
    img = np.expand_dims(img, axis=0)
    img = preprocess_input(img)
    return face_model.predict(img)[0]

# ------------------- Voice Utilities ----------------------
def record_voice(filename='temp.wav', duration=5, sr=16000):
    st.write("Recording...")
    audio = sd.rec(int(duration * sr), samplerate=sr, channels=1)
    sd.wait()
    sf.write(filename, audio, sr)
    st.write("Recording complete.")

def extract_voice_features(file):
    y, sr = librosa.load(file, sr=16000)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
    return mfcc.T[:100]  # Fixed length

# -------------------- Model Training ----------------------
def train_voice_model():
    X, y = [], []
    for fname in os.listdir(DATA_VOICE):
        if fname.endswith('.npy'):
            X.append(np.load(os.path.join(DATA_VOICE, fname)))
            y.append(fname.split("_")[0])

    X = np.array(X)
    y = np.array(y)

    le = LabelEncoder()
    y = le.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    model = Sequential([
        LSTM(128, input_shape=(X.shape[1], X.shape[2])),
        Dense(64, activation='relu'),
        Dense(len(set(y)), activation='softmax')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=8)
    model.save(MODEL_PATH)

if not os.path.exists(MODEL_PATH):
    train_voice_model()

voice_model = load_model(MODEL_PATH)

# -------------------- Verification ----------------------
def verify_face(stored_embedding, current_embedding, threshold=0.6):
    similarity = cosine_similarity([stored_embedding], [current_embedding])[0][0]
    return similarity, similarity >= threshold

def verify_voice(stored_feat, current_feat, threshold=0.6):
    pred1 = np.mean(voice_model.predict(np.expand_dims(stored_feat, axis=0)))
    pred2 = np.mean(voice_model.predict(np.expand_dims(current_feat, axis=0)))
    similarity = 1 - abs(pred1 - pred2)
    return similarity, similarity >= threshold

# ---------------------- Streamlit UI ----------------------
st.title("\U0001F475 Elderly Biometric Authentication System")

option = st.radio("Choose Operation", ["Enroll User", "Authenticate User"])
name = st.text_input("Enter User Name:")

if option == "Enroll User" and name:
    if st.button("Capture Face"):
        cap = cv2.VideoCapture(0)
        st.write("Press space to capture...")
        while True:
            ret, frame = cap.read()
            cv2.imshow('Face Capture', frame)
            if cv2.waitKey(1) & 0xFF == ord(' '):
                face_path = f"{DATA_FACE}/{name}.jpg"
                cv2.imwrite(face_path, frame)
                cv2.destroyAllWindows()
                break
        cap.release()
        st.success("Face Captured!")

    if st.button("Capture Voice"):
        voice_path = f"{DATA_VOICE}/{name}_features.npy"
        record_voice('temp.wav')
        feat = extract_voice_features('temp.wav')
        np.save(voice_path, feat)
        st.success("Voice Captured!")

elif option == "Authenticate User" and name:
    if st.button("Verify Identity"):
        try:
            stored_face = extract_face_embedding(f"{DATA_FACE}/{name}.jpg")
            cap = cv2.VideoCapture(0)
            st.write("Press space to capture your face")
            while True:
                ret, frame = cap.read()
                cv2.imshow('Verify Face', frame)
                if cv2.waitKey(1) & 0xFF == ord(' '):
                    cv2.imwrite('temp_auth.jpg', frame)
                    cv2.destroyAllWindows()
                    break
            cap.release()
            live_face = extract_face_embedding('temp_auth.jpg')
            face_sim, face_pass = verify_face(stored_face, live_face)

            stored_voice_feat = np.load(f"{DATA_VOICE}/{name}_features.npy")
            record_voice('temp.wav')
            auth_feat = extract_voice_features('temp.wav')
            voice_sim, voice_pass = verify_voice(stored_voice_feat, auth_feat)

            st.write(f"Face Similarity: {face_sim:.2f}")
            st.write(f"Voice Similarity: {voice_sim:.2f}")

            if face_pass and voice_pass:
                st.success("\u2705 Authentication Successful!")
            else:
                st.error("\u274C Authentication Failed.")

        except Exception as e:
            st.error(f"Error: {str(e)}")
